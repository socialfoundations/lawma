{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-4 zero shot and few shot\n",
    "\n",
    "Compute the GPT-4 zero-shot and few-shot accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import load_responses, responses_to_acc, majority_acc, compute_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 4540.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4 has 260 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 7970.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt4-fewshot has 260 tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "responses = load_responses({}, ['gpt4', 'gpt4-fewshot'], base_dir='results/model_responses/')\n",
    "responses['gpt4-2'] = responses['gpt4']  # just a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:00<00:00, 72706.12it/s]\n",
      "100%|██████████| 260/260 [00:00<00:00, 187890.94it/s]\n",
      "100%|██████████| 260/260 [00:00<00:00, 14666.19it/s]\n"
     ]
    }
   ],
   "source": [
    "accs = {}\n",
    "for model, model_responses in responses.items():\n",
    "    accs[model] = {}\n",
    "    for file, response in tqdm(model_responses.items()):\n",
    "        if model == 'gpt4-2':\n",
    "            accs[model][file] = responses_to_acc(response, gpt4=True)\n",
    "        else:\n",
    "            # evaluated on the very same examples as gpt-4 few-shot\n",
    "            # note that we evaluate gpt-4 few-shot on fewer examples due to the much higher cost of evaluation\n",
    "            accs[model][file] = responses_to_acc(response[:5], gpt4=True)\n",
    "accs['maj'] = {k: majority_acc(v) for k, v in list(responses.values())[0].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_issue and sc_casetyp1 are the overage of certain tasks\n",
    "tasks_to_average = {\n",
    "    'sc_issue_': 'sc_issue',\n",
    "    'songer_casetyp1_': 'songer_casetyp1',\n",
    "}\n",
    "averages = compute_averages(responses, tasks_to_average)\n",
    "for model in accs.keys():\n",
    "    for task in tasks_to_average.values():\n",
    "        accs[model][task] = averages[model][task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- All\n",
      "  * GPT4 32k few-shot:                 0.5838\n",
      "  * GPT4 8k zero-shot (same examples): 0.6161\n",
      "  * GPT4 8k zero-shot (all examples):  0.6289\n",
      "- Songer\n",
      "  * GPT4 32k few-shot:                 0.5912\n",
      "  * GPT4 8k zero-shot (same examples): 0.6284\n",
      "  * GPT4 8k zero-shot (all examples):  0.6342\n",
      "- SC\n",
      "  * GPT4 32k few-shot:                 0.5399\n",
      "  * GPT4 8k zero-shot (same examples): 0.5436\n",
      "  * GPT4 8k zero-shot (all examples):  0.5978\n"
     ]
    }
   ],
   "source": [
    "prefixes = {\n",
    "    '': 'All',\n",
    "    'songer_': 'Songer',\n",
    "    'sc_': 'SC',\n",
    "}\n",
    "\n",
    "for prefix, prefix_name in prefixes.items():\n",
    "    accs1 = []\n",
    "    accs2 = []\n",
    "    accs3 = []\n",
    "\n",
    "    for task, v in accs['gpt4-fewshot'].items():\n",
    "        if task.startswith(prefix):\n",
    "            accs1.append(v)\n",
    "            accs2.append(accs['gpt4'][task])\n",
    "            accs3.append(accs['gpt4-2'][task])\n",
    "\n",
    "    print(f'- {prefix_name}')\n",
    "    print(f'  * GPT4 32k few-shot:                 {np.mean(accs1):.4f}')\n",
    "    print(f'  * GPT4 8k zero-shot (same examples): {np.mean(accs2):.4f}') \n",
    "    print(f'  * GPT4 8k zero-shot (all examples):  {np.mean(accs3):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** These numbers compute GPT-4 zero shot performance on the same examples as the GPT few shot evaluations. This is a subset of all cases. The line `all examples` refers to the performance on all examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
